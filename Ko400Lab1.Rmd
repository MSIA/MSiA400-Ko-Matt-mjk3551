---
title: |
  | \Huge MSiA 400 Lab Assignment 1: mjk3551
author: |
  | \Large
  | \Large Matt Ko
output:
  pdf_document: default
  word_document: default
---
```{r setup, include=FALSE, message = FALSE, warning=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 999)
```


## Problem 1a

### Construct a 9 by 9 matrix `Traffic` that counts total traffic from State $i$ to State $j$, for $i,j\in\{1,\cdots,9\}$. Note that `Traffic` has 0's in row 9 and column 1. Set `Traffic[9,1]=1000`. (This is equivalent to making each user return to the home page after they leave the website.) Display `Traffic`. \underline{Hint:} `colSums()` adds all rows for each column.

```{r}
web = as.matrix(read.delim("webtraffic.txt"))
counts = colSums(web)
Traffic = matrix(,nrow = 9,ncol = 9)
for(row in 1:9){
  for(col in 1:9){
    #print((row-1)+(col))
    #print(counts[(row-1)*9+(col)])
    Traffic[row,col]=counts[(row-1)*9+(col)]
  }
}
Traffic[9,1]=1000
Traffic
```

## Problem 1b
### Draw a directed graph where each node represents a state, and each arrow from State $i$ to State $j$ has positive (non-zero) traffic (i.e., `Traffic[i,j]>0`). This may be submitted as a TikZ graph (or using your graphing program of choice) or a picture of a hand-drawn graph (provided it is legible). Is the Markov chain irreducible? Is the Markov chain ergodic? Explain.


!['Markov Graph'](MarkovGraph.jpg)


### This graph is both irreducible and ergodic. It is irreducible because all states communicate with each other. In terms of this problem, you can get to any page after entering the website through the website without entering a url separately. The graph it ergodic because all states are aperiodic, with most having traffic from itself to itself, and the greatest common factor of the number of steps needed to get back to state 1 and 9 is 1. All states are also recurrent because they will eventually return to the same state if they leave. 


## Problem 1c
### Construct and display the one-step transition probability matrix `P` (using the Maximum Likelihood estimate, i.e., $p_{ij}=\frac{\text{Traffic}[i,j]}{\sum\limits_{j=1}^9\text{Traffic}[i,j]}$).

```{r}
rowcount = rowSums(Traffic)
ProbMatrix = matrix(,nrow = 9,ncol = 9)
for(row in 1:9){
  for(col in 1:9){
    ProbMatrix[row,col] = Traffic[row,col]/rowcount[row]
  }
}
ProbMatrix
```

## Problem 1d
### What is the probability of a visitor being on Page 5 after 5 clicks?


```{r}
a = c(1,rep(0,8))
a
prob5 = a %*% ProbMatrix %*% ProbMatrix %*% ProbMatrix %*% ProbMatrix %*% ProbMatrix
prob5[5]
```

### ~13.15% probability that the visitor is on page 5 after 5 clicks.

## Problem 1e
### Compute and display the steady-state probability vector `Pi`, solving the system of equations (as demonstrated in lab).


```{r}
ProbMatrix[9,1]=1; ProbMatrix[9,9]=0 #make the network irreducible
Q=t(ProbMatrix)-diag(9)
Q[9,]=rep(1,9)
rhs=c(rep(0,8),1)
Pi=solve(Q,rhs) 
Pi
```


## Problem 1f

### What is the average time a visitor spends on the website (until he/she first leaves)? \underline{Hint:} Modify the mean first passage time equations, with time spent at each state.

```{r}
B=ProbMatrix[1:8,1:8]
Q=diag(8)-B
rhs = c(0.1, 2, 3, 5, 5, 3, 3, 2)
rhs
m=solve(Q,rhs)
m[1]
```

### 14.563 minutes.

## Problem 2a
### Determine the number of samples required to achieve an error tolerance of $10^{-3}$ with 99\% confidence.


```{r}
getsamp <- function(lambda,delta,tolerance){
  n = (1/(lambda^2))/((tolerance)^2*(1-delta))
  return(n)
}
```

\[(1/\lambda^2)/(10^{-3})^2 *(1-0.99) = 10^6/(0.01*\lambda^2)\]

The expression above is the expression regarding the number of samples needed with respect to lambda. 

## Problem 2b
### Compute the approximation (using the number of samples obtained in Problem 2a) and verify that it is within tolerance by comparing to the exact solution: $\int\limits_0^\infty e^{-\lambda x} \sin x dx=\frac{1}{1+\lambda^2}$. Numerically evaluate for each of $\lambda=1,2,4$.


```{r}
set.seed(100)
approx <- function(lambda,n){
  
  X = runif(n,0,1)
  Y = -log(X)/lambda
  z = sin(Y)/lambda
  ans = sum(z)/n
  return(ans)
}

#for lambda = 1
n1 = getsamp(1,0.99,10^-3)
l1 = approx(1,n1)
l1

#for lambda = 2

n2 = getsamp(2,0.99,10^-3)
l2 = approx(2,n2)
l2

#for lambda = 4

n4 = getsamp(4,0.99,10^-3)
l4 = approx(4,n4)
l4

```



```{r}
#exact solution comparison

exact <- function(lambda){
  
  ans = 1/(1+lambda^2)
  return(ans)
}

#for lambda=1
print(exact(1))

#for lambda=2
print(exact(2))

#for lambda=4
print(exact(4))

```

## Problem 3a

### Which MCMC algorithm (Metropolis, Metropolis-Hastings, or Gibbs) is better suited for this problem?


### Metropolis-Hastings is the best, because the distribution is not large and multivariate and the exponential distribution is not symmetric. 

## Problem 3b

### Using a burn-in period of 5000 samples and keeping every 100 samples, generate 100 samples from the gamma distribution with shape $k=2$ and scale $\theta=2$. Use the algorithm you chose in Problem 3a and write your own sampler (as opposed to using a function from a package).

```{r}
x0 = 1

q_func = function(x){
  
  ans = rexp(1,rate = x)
  return(ans)
  
}

p_func = function(x,k,theta){
  ans = x^(k-1)*exp(-1*x/theta)
}

algo_func = function(t,k,thet){
  x = rep(NA,t)
  x[1]=1
  for(i in 1:t){
    xp = q_func(x[i])
    alpha = (p_func(xp,k,thet)*q_func(x[i]))/(p_func(x[i],k,thet)*q_func(xp))
    u = runif(1,0,1)
    if(u<=alpha){
      x[i+1] = xp
    }
    else{
      x[i+1] = x[i]
    }
  }
  return(x)
}

x =algo_func(15000,2,2)
test <- x[5001:15000]
ans <- test[seq(1, length(test), 100)]
ans
```

## Problem 3c
### Are the samples generated in Problem 3b sufficiently random? How can you tell?

```{r}
hist(ans,15,prob=TRUE)
curve(dgamma(x, shape = 2, scale = 2), add=TRUE,col="red")
```

```{r}
stepvector = seq(1,100)
cor(ans,stepvector)
```

```{r}
plot(stepvector,ans)
```

```{r}
acf(ans)
```

### Overall, based on the histogram and the plot of steps versus values, the values visually seem sufficiently random. The correlation between the steps and values is also very low, it can be considered negligible. Additionally, the ACF function does not suggest that there exists autocorrelation within our samples. 
